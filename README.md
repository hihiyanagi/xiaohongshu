# Xiaohongshu Scraper API

A FastAPI-based service for extracting content from Xiaohongshu articles using either direct URLs or share text.

## Features

- Extract titles, content, and images from Xiaohongshu articles
- Support for extracting links from Xiaohongshu share text
- Smart URL detection: provide either direct URL or share text in the URL field
- Meta tag-based extraction for more reliable content parsing
- Clean and well-structured API responses
- Error handling for various request scenarios

## Installation

1. Clone this repository
2. Install the required dependencies:

```bash
pip install fastapi uvicorn requests beautifulsoup4
```

3. Run the API server:

```bash
python xiaohongshu_api.py
```

The server will start at `http://localhost:8000`.

## API Endpoints

### 1. URL-based Scraping

**Endpoint:** `/scrape`

**Method:** POST

**Request Body:** 

You can use this endpoint in two ways:

#### Option 1: Direct URL

```json
{
    "url": "https://www.xiaohongshu.com/explore/XXXXXX",
    "timeout": 10,
    "use_meta_tags": true
}
```

#### Option 2: Share Text (Smart Detection)

```json
{
    "url": "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯...",
    "timeout": 10,
    "use_meta_tags": true
}
```

The API will automatically detect that this is share text and extract the link from it.

Parameters:
- `url` (required): The Xiaohongshu article URL or share text containing a URL
- `timeout` (optional): Request timeout in seconds (default: 10)
- `use_meta_tags` (optional): Whether to use meta tags for extraction (default: false)

### 2. Share Text-based Scraping

**Endpoint:** `/scrape-share`

**Method:** POST

**Request Body:**

```json
{
    "share_text": "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ğŸ˜† jae0CHZjV76kRRt ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯...",
    "timeout": 10,
    "use_meta_tags": true
}
```

Parameters:
- `share_text` (required): The text shared from Xiaohongshu app that contains the link
- `timeout` (optional): Request timeout in seconds (default: 10)
- `use_meta_tags` (optional): Whether to use meta tags for extraction (default: true)

### Response Format

Both endpoints return the same response format:

```json
{
    "title": "Article Title",
    "subtitle": null,
    "content": ["Paragraph 1", "Paragraph 2", "..."],
    "images": ["image_url_1", "image_url_2", "..."],
    "success": true,
    "message": "Content successfully scraped",
    "extracted_link": "http://xhslink.com/a/..." // URL extracted from share text (null if direct URL was provided)
}
```

## Example Usage

### Using Python Requests

```python
import requests

# Option 1: URL-based scraping
scrape_data = {
    "url": "https://www.xiaohongshu.com/explore/XXXXXX",
    "use_meta_tags": True
}
response = requests.post("http://localhost:8000/scrape", json=scrape_data)
result = response.json()

# Option 2: Direct share text in URL field (simplest approach)
scrape_data = {
    "url": "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯...",
    "use_meta_tags": True
}
response = requests.post("http://localhost:8000/scrape", json=scrape_data)
result = response.json()

# Option 3: Share text-based scraping (dedicated endpoint)
share_data = {
    "share_text": "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯...",
    "use_meta_tags": True
}
response = requests.post("http://localhost:8000/scrape-share", json=share_data)
result = response.json()
```

### Test Scripts

This repository includes test scripts to demonstrate the API:

- `test_smart_scrape.py`: Tests the smart URL detection that handles share text directly in URL field
- `test_share_text.py`: Tests the dedicated share text endpoint
- `test_html_api.py`: Tests direct URL scraping with meta tag extraction

Run the tests:

```bash
python test_smart_scrape.py
```

## Command-line Tool

For quick testing, you can use the included `extract_link.py` command-line tool:

```bash
# Extract link from share text
python extract_link.py "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡"

# Extract link and test by fetching content
python extract_link.py -t "å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡"

# Read share text from file
python extract_link.py -f share_text.txt -t
```

## Share Text Link Extraction

The API can extract Xiaohongshu links from share text like this:

```
40 å°å±å­©é™¶å¾·å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† jae0CHZjV76kRRt ğŸ˜† http://xhslink.com/a/JEF7CXDbSuFabï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯ï¼Œæ‰“å¼€ã€å°çº¢ä¹¦ã€‘AppæŸ¥çœ‹ç²¾å½©å†…å®¹ï¼
```

It automatically finds and extracts the `http://xhslink.com/a/JEF7CXDbSuFab` link, and then uses it to scrape the content.

## Example for HTML Meta Tags

Here's an example of the meta tags structure this API can parse:

```html
<meta property="og:title" content="11/8æˆ‘çš„çˆµå£«ä¸‰é‡å¥åœ¨è»çªªvelvet sunæ¼”å‡º - å°çº¢ä¹¦">
<meta name="description" content="open 7:15- start 8:00- æ¬¢è¿æœ‹å‹ä»¬æ¥èšï¼...">
<meta property="og:image" content="http://sns-webpic-qc.xhscdn.com/...">
<meta property="og:image" content="http://sns-webpic-qc.xhscdn.com/...">
```

## License

[MIT](LICENSE)

# YouTube API - Verceléƒ¨ç½²æŒ‡å—

è¿™ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºPythonçš„YouTube APIæœåŠ¡ï¼Œå¯ä»¥éƒ¨ç½²åˆ°Vercelä¸Šä½œä¸ºServerless Functionsè¿è¡Œã€‚

## åŠŸèƒ½

é€šè¿‡REST APIæä¾›ä»¥ä¸‹åŠŸèƒ½:

- æœç´¢YouTubeè§†é¢‘
- è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
- è·å–é¢‘é“ä¿¡æ¯
- è·å–è§†é¢‘è¯„è®º

## å‡†å¤‡å·¥ä½œ

1. **è·å–YouTube APIå¯†é’¥**

   æŒ‰ç…§ [youtube_api_guide.md](youtube_api_guide.md) ä¸­çš„è¯´æ˜è·å–YouTube Data API v3å¯†é’¥ã€‚

2. **å®‰è£…Vercel CLI**

   ```bash
   npm install -g vercel
   ```

## æœ¬åœ°å¼€å‘

1. **å®‰è£…ä¾èµ–**

   ```bash
   pip install -r requirements.txt
   ```

2. **è®¾ç½®ç¯å¢ƒå˜é‡**

   å¤åˆ¶`.env.example`æ–‡ä»¶å¹¶é‡å‘½åä¸º`.env`ï¼Œç„¶åæ·»åŠ ä½ çš„YouTube APIå¯†é’¥ï¼š

   ```bash
   cp .env.example .env
   # ç¼–è¾‘.envæ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„APIå¯†é’¥
   ```

3. **æœ¬åœ°è¿è¡ŒAPI**

   ```bash
   cd api
   uvicorn index:app --reload
   ```

   ç°åœ¨ä½ å¯ä»¥è®¿é—® http://localhost:8000 æ¥ä½¿ç”¨APIã€‚
   
   è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£ç•Œé¢ã€‚

## éƒ¨ç½²åˆ°Vercel

1. **ç™»å½•Vercel**

   ```bash
   vercel login
   ```

2. **éƒ¨ç½²é¡¹ç›®**

   ```bash
   vercel
   ```

   æŒ‰ç…§æç¤ºæ“ä½œã€‚ç¡®ä¿åœ¨Vercelè®¾ç½®ä¸­æ·»åŠ ç¯å¢ƒå˜é‡`YOUTUBE_API_KEY`ã€‚

3. **è®¾ç½®ç¯å¢ƒå˜é‡**

   ä½ å¯ä»¥åœ¨Vercelä»ªè¡¨ç›˜ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæˆ–è€…ä½¿ç”¨CLI:

   ```bash
   vercel env add YOUTUBE_API_KEY
   ```

4. **é‡æ–°éƒ¨ç½²ï¼ˆå¦‚æœéœ€è¦ï¼‰**

   ```bash
   vercel --prod
   ```

## APIç«¯ç‚¹

éƒ¨ç½²åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ç«¯ç‚¹:

- `GET /` - æ¬¢è¿ä¿¡æ¯å’ŒAPIç‰ˆæœ¬
- `GET /search?query=å…³é”®è¯&max_results=5&language=zh-CN` - æœç´¢è§†é¢‘
- `GET /videos/{video_id}` - è·å–å•ä¸ªè§†é¢‘ä¿¡æ¯
- `GET /videos?video_ids=id1,id2,id3` - è·å–å¤šä¸ªè§†é¢‘ä¿¡æ¯
- `GET /channels/{channel_id}` - è·å–é¢‘é“ä¿¡æ¯
- `GET /videos/{video_id}/comments?max_results=10` - è·å–è§†é¢‘è¯„è®º

## æ³¨æ„äº‹é¡¹

- Vercelå…è´¹è®¡åˆ’æœ‰ä¸€äº›é™åˆ¶ï¼ŒåŒ…æ‹¬å‡½æ•°æ‰§è¡Œæ—¶é—´å’Œéƒ¨ç½²å¤§å°
- YouTube APIæœ‰é…é¢é™åˆ¶ï¼Œè¯·å‚è€ƒ[é…é¢å’Œé™åˆ¶](https://developers.google.com/youtube/v3/getting-started#quota)
- ç¡®ä¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®é€‚å½“çš„CORSç­–ç•¥å’ŒAPIå¯†é’¥ä¿æŠ¤

## ä½¿ç”¨ç¤ºä¾‹

ä½¿ç”¨curlè·å–è§†é¢‘ä¿¡æ¯:

```bash
curl https://ä½ çš„vercelåŸŸå.vercel.app/videos/VIDEO_ID
```

æœç´¢è§†é¢‘:

```bash
curl https://ä½ çš„vercelåŸŸå.vercel.app/search?query=ç¼–ç¨‹æ•™ç¨‹&max_results=3
``` 